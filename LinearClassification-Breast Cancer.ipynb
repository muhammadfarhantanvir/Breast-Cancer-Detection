{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"LinearClassification-Breast Cancer.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"q9AAjWI7BDZb","colab_type":"code","colab":{}},"source":["#Linear Classification / Logistic Regression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GnEqToyBDZg","colab_type":"code","colab":{}},"source":["#importing lib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ty4x6_ZdBDZj","colab_type":"code","colab":{}},"source":["class LinearClassification(object):\n","    \n","    def __init__(self):\n","        #defining hyperparams\n","        self.learning_rate = 0.0001\n","        self.batch_size = 200\n","        self.no_of_iter = 1000\n","        #videcu za ovaj\n","        self.reg = 0.000001\n","        \n","    \n","    #Input NOTE: X - matrix of data, can be used on images or numerical data (N x D)\n","    #          N - Number of samples, D - Number of features\n","    #          In case you use images make sure that X.shape[0] represent NUMBER of samples\n","    #          y - labels (Nx1)\n","    def fit(self, X, y):\n","        self.X_train = X\n","        self.y_train = y\n","        \n","        #0 notation - so we add + 1 to max value from y\n","        self.no_of_classes = np.max(y) + 1\n","        \n","        #defining hyperparams\n","        # W - matrix of weights (No_of_classes x No_of_features)\n","        self.W = np.random.rand(self.no_of_classes, self.X_train.shape[1]) * 0.001\n","        \n","        self.W, loss_history = self.SGD(self.W, self.X_train, self.y_train, self.learning_rate, self.batch_size, self.no_of_iter, self.reg)\n","        \n","        return loss_history\n","    \n","    #STOCHASTIC GRADIENT DESCENT\n","    #Inputs: W - weights that we are trying to update\n","    #        X - feautere of training set\n","    #        y - wanted labels\n","    #        learning_rate - how fast it is going to find good parameters\n","    #        batch_size - how big PART of training set algo is using per iter\n","    #        no_of_iter -  how many times it is going to run\n","    #        reg - regularization\n","    #\n","    #Outputs: W_updated - updated weights matrix acording to loss function used\n","    #         loss_history for verbose reptresentation of our loss computation\n","    def SGD(self, W, X, y, learning_rate, batch_size, no_of_iter, reg):\n","        W_updated = W\n","        \n","        no_of_train = X.shape[0]\n","        #It is not necessities, but we can define loss_hitory to be sure that algo is working good\n","        loss_history = []\n","        \n","        for i in range(no_of_iter):\n","            batch_inx = np.random.choice(no_of_train, batch_size, replace=True)\n","            #creting smallers train sets to fit in our SGD\n","            X_batch = X[batch_inx,:]\n","            y_batch = y[batch_inx]\n","            \n","            \n","            loss, grad = self.SVM_classfier(W_updated, X_batch, y_batch, reg)\n","            loss_history.append(loss)\n","            #Update W:\n","            W_updated = W_updated - (learning_rate * grad)\n","            \n","        return W_updated, loss_history\n","            \n","    #Inputs: W - current weights\n","    #        X - training set features\n","    #        y - training set labels\n","    #        reg - regularization strenght\n","    #\n","    #Outputs: gradient_W - values to updated starting W\n","    #         loss - to see if we are updaing in good direction\n","    def SVM_classfier(self, W, X, y, reg):\n","        \n","        no_of_classes = np.max(y) + 1\n","        #creating matrix with zeros, same shape as starting weights\n","        \n","        gradient_W = np.zeros(W.shape)\n","        \n","        loss = 0.0 \n","        for i in range(X.shape[0]):\n","            #First we need to multiply weights and x for particular sample\n","            #need to transpose to long vector current sample\n","            scores = W.dot(X[i, :].T)\n","            #we are getting values for currect class\n","            correct_class = scores[y[i]]\n","            for j in range(no_of_classes):\n","                if j == y[i]:\n","                    continue\n","                # This is simple formula for SVM\n","                current_class_margin = scores[j] - correct_class + 1 #one is \n","                if current_class_margin > 0:\n","                    loss +=  current_class_margin\n","                \n","                    gradient_W[y[i]:1, :] -= X[i, :] #This is where we are creating gradient for CURRECT class\n","                    gradient_W[j:1, :] += X[y[i], :]\n","        \n","        #average over number of train samples\n","        loss /= X.shape[0]\n","        gradient_W /= X.shape[0]\n","        \n","        loss += 0.5 * reg * np.sum(W * W)\n","        \n","        gradient_W += reg*W\n","    \n","        return loss, gradient_W\n","    \n","    #Predict function\n","    #Input: X - test set \n","    #\n","    #Output: predict - list of classes\n","    def predict(self, X):\n","        pred = []\n","        for i in range(X.shape[0]):\n","            pred.append(np.argmax(np.dot(self.W,X[i, :].T)))\n","        return pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOmoOQZFBDZl","colab_type":"code","colab":{}},"source":["#to check how much did algo predict right\n","def accuracy(y_tes, y_pred):\n","    correct = 0\n","    for i in range(len(y_pred)):\n","        if(y_tes[i] == y_pred[i]):\n","            correct += 1\n","    return (correct/len(y_tes))*100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFNHL24EBDZn","colab_type":"code","colab":{}},"source":["def run():\n","    # Importing the dataset\n","    dataset = pd.read_csv('breastCancer.csv')\n","    dataset.replace('?', 0, inplace=True)\n","    dataset = dataset.applymap(np.int64)\n","    X = dataset.iloc[:, 1:-1].values    \n","    y = dataset.iloc[:, -1].values\n","    #handling labels column\n","    y_new = []\n","    for i in range(len(y)):\n","        if y[i] == 2:\n","            y_new.append(0)\n","        else:\n","            y_new.append(1)\n","    y_new = np.array(y_new)\n","\n","    \n","    # Splitting the dataset into the Training set and Test set\n","    from sklearn.model_selection import train_test_split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size = 0.25, random_state = 0)\n","    \n","\n","    # Feature Scaling\n","#     from sklearn.preprocessing import StandardScaler\n","#     sc = StandardScaler()\n","#     X_train = sc.fit_transform(X_train)\n","#     X_test = sc.transform(X_test)\n","\n","   \n","    classifier = LinearClassification()\n","    loss_history = classifier.fit(X_train, y_train)\n","    \n","    y_pred = classifier.predict(X_test)\n","    \n","    #Sklearn test\n","    from sklearn.linear_model import LogisticRegression\n","    reg = LogisticRegression(random_state=0)\n","    reg.fit(X_train, y_train)\n","    \n","    y_pred_sk = reg.predict(X_test)\n","\n","# Uncomment if you want to print out losses\n","#     for i in range(len(loss_history)):\n","#         print(loss_history[i])\n","    \n","    print(\"My algorithm on this dataset: \",accuracy(y_test, y_pred), \"%\")\n","    print(\"Sklearn Logistic regression score: \",accuracy(y_test, y_pred_sk),\"%\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8e-63o5BDZq","colab_type":"code","colab":{},"outputId":"10f96c4c-29f4-457f-ce13-81eec6ad2196"},"source":["run()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["My algorithm on this dataset:  65.71428571428571 %\n","Sklearn Logistic regression score:  96.57142857142857 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A4_EvPkCBDZu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HdI7P9iBDZw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdM4_m1dBDZy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}